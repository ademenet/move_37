{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cart pole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The toy example from [gym](https://gym.openai.com/docs/#installation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_episode in range(20):\n",
    "    observation = env.reset()\n",
    "    for t in range(100):\n",
    "        # To display the environment\n",
    "        env.render()\n",
    "        print(observation)\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(f\"Episode finished after {t+1} timesteps\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The examples from [kvfrans.com](http://kvfrans.com/simple-algoritms-for-solving-cartpole/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, parameters):\n",
    "    observation = env.reset()\n",
    "    totalreward = 0\n",
    "    for _ in range(200):\n",
    "        action = 0 if np.matmul(parameters, observation) < 0 else 1\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        totalreward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return totalreward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random search\n",
    "\n",
    "We are going to run several tests with different weights initalization and pick up the ones with the highest total reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestparams = None\n",
    "bestreward = 0\n",
    "for i in range(10000):\n",
    "    # Initialize a vector of random weights for each observations (4 for CartPole)\n",
    "    parameters = np.random.rand(4) * 2 - 1\n",
    "    reward = run_episode(env, parameters)\n",
    "    if reward > bestreward:\n",
    "        bestreward = reward\n",
    "        bestparams = parameters\n",
    "        if reward == 200:\n",
    "            print(f\"Stopped at iteration {i}\")\n",
    "            break\n",
    "print(f\"Best reward: {bestreward} with best parameters: {bestparams}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hill climbing\n",
    "\n",
    "This technique use a trick to avoid testing random weights all the time. Here you intialize randomly the weights then add some noise to the (best) weights. It improves all the time but it could get stucked finding nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_scaling = 0.1\n",
    "parameters = np.random.rand(4) * 2 - 1\n",
    "bestreward = 0\n",
    "for i in range(10000):\n",
    "    newparams = parameters + (np.random.rand(4) * 2 - 1) * noise_scaling\n",
    "    reward = 0\n",
    "    reward = run_episode(env, newparams)\n",
    "    if reward > bestreward:\n",
    "        bestreward = reward\n",
    "        parameters = newparams\n",
    "        if reward == 200:\n",
    "            print(f\"Stopped at iteration {i}\")\n",
    "            break\n",
    "print(f\"Best reward: {bestreward} with best parameters: {parameters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_gradient():\n",
    "    params = tf.get_variable(\"policy_parameters\", [4,2])\n",
    "    state = tf.placeholder(\"float\", [None,4])\n",
    "    actions = tf.placeholder(\"float\", [None,2])\n",
    "    linear = tf.matmul(state, params)\n",
    "    probabilities = tf.nn.softmax(linear)\n",
    "    good_probabilities = tf.reduce_sum(tf.matmul(probabilities, actions), reduction_indices=[1])\n",
    "    # Maximize the log probabilty\n",
    "    log_probabilities = tf.log(good_probabilities)\n",
    "    loss = -tf.reduce_sum(log_probabilities)\n",
    "    optimizer = tf.train.AdamOptimizer(0.01).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_gradient():  \n",
    "    # sess.run(calculated) to calculate value of state\n",
    "    state = tf.placeholder(\"float\",[None,4])\n",
    "    w1 = tf.get_variable(\"w1\",[4,10])\n",
    "    b1 = tf.get_variable(\"b1\",[10])\n",
    "    h1 = tf.nn.relu(tf.matmul(state,w1) + b1)\n",
    "    w2 = tf.get_variable(\"w2\",[10,1])\n",
    "    b2 = tf.get_variable(\"b2\",[1])\n",
    "    calculated = tf.matmul(h1,w2) + b2\n",
    "\n",
    "    # sess.run(optimizer) to update the value of a state\n",
    "    newvals = tf.placeholder(\"float\",[None,1])\n",
    "    diffs = calculated - newvals\n",
    "    loss = tf.nn.l2_loss(diffs)\n",
    "    optimizer = tf.train.AdamOptimizer(0.1).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow operations to compute probabilties for each action, given a state\n",
    "pl_probabilities, pl_state = policy_gradient()  \n",
    "observation = env.reset()  \n",
    "actions = []  \n",
    "transitions = []  \n",
    "for _ in xrange(200):  \n",
    "    # calculate policy\n",
    "    obs_vector = np.expand_dims(observation, axis=0)\n",
    "    probs = sess.run(pl_probabilities,feed_dict={pl_state: obs_vector})\n",
    "    action = 0 if random.uniform(0,1) < probs[0][0] else 1\n",
    "    # record the transition\n",
    "    states.append(observation)\n",
    "    actionblank = np.zeros(2)\n",
    "    actionblank[action] = 1\n",
    "    actions.append(actionblank)\n",
    "    # take the action in the environment\n",
    "    old_observation = observation\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    transitions.append((old_observation, action, reward))\n",
    "    totalreward += reward\n",
    "\n",
    "    if done:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
